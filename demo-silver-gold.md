Voici un **pipeline complet en YAML** pour **Azure DevOps**, qui orchestre l'ex√©cution de tes notebooks dans **Microsoft Fabric**, le stockage des donn√©es dans **Lakehouse**, la publication du rapport **Power BI** et le rafra√Æchissement des donn√©es.

---

### **üìå Fichier YAML : `azure-pipelines.yml`**
Ce fichier contient :
1. **Ex√©cution des Notebooks Fabric** (**Raw ‚Üí Landing ‚Üí Bronze ‚Üí Silver ‚Üí Gold**).
2. **Chargement des donn√©es dans Microsoft Fabric Lakehouse**.
3. **Publication du rapport Power BI (`LMS_import.pbix`)**.
4. **Rafra√Æchissement des donn√©es dans Power BI**.

---

### **üí° Pr√©-requis**
- üîë **Cr√©e une Variable Group dans Azure DevOps** avec :
  - `FABRIC_ACCESS_TOKEN` ‚Üí Token d‚Äôauthentification Fabric.
  - `PBI_ACCESS_TOKEN` ‚Üí Token d‚Äôauthentification Power BI.
  - `FABRIC_WORKSPACE_ID` ‚Üí ID du workspace Fabric.
  - `FABRIC_LAKEHOUSE_ID` ‚Üí ID du Lakehouse.
  - `FABRIC_NOTEBOOK_IDS` ‚Üí Liste des IDs des notebooks Fabric.
  - `PBI_DATASET_ID` ‚Üí ID du dataset Power BI.
- üìå **Stocke ton fichier PBIX (`LMS_import.pbix`)** dans ton repo Git.

---

## **üöÄ Pipeline YAML**
```yaml
trigger:
  branches:
    include:
      - main  # Ex√©cute la pipeline sur les commits sur main

schedules:
  - cron: "0 2 * * *"  # Planifie l‚Äôex√©cution tous les jours √† 2h du matin
    displayName: "Run daily at 2 AM"
    branches:
      include:
        - main

stages:
  - stage: TransformData
    displayName: "1Ô∏è‚É£ Ex√©cution des notebooks Fabric"
    jobs:
      - job: RunNotebooks
        displayName: "Ex√©cuter les transformations des donn√©es"
        steps:
          - script: |
              NOTEBOOK_IDS=($(echo "$(FABRIC_NOTEBOOK_IDS)" | tr "," "\n"))
              for notebook in "${NOTEBOOK_IDS[@]}"; do
                curl -X POST "https://api.fabric.microsoft.com/v1/notebooks/$notebook/execute" \
                  -H "Authorization: Bearer $(FABRIC_ACCESS_TOKEN)" \
                  -H "Content-Type: application/json" \
                  -d '{ "parameters": {} }'
                echo "Notebook $notebook ex√©cut√©"
              done
            displayName: "Ex√©cuter les Notebooks Fabric"

  - stage: StoreData
    displayName: "2Ô∏è‚É£ Stockage des donn√©es dans Fabric Lakehouse"
    jobs:
      - job: UploadToLakehouse
        displayName: "Charger les donn√©es dans OneLake"
        steps:
          - script: |
              curl -X POST "https://api.fabric.microsoft.com/v1/lakehouses/$(FABRIC_LAKEHOUSE_ID)/upload" \
                   -H "Authorization: Bearer $(FABRIC_ACCESS_TOKEN)" \
                   -F "file=@data/output.parquet"
            displayName: "Envoyer les donn√©es transform√©es"

  - stage: DeployPowerBI
    displayName: "3Ô∏è‚É£ D√©ploiement du rapport Power BI"
    jobs:
      - job: PublishPBIX
        displayName: "Publier le rapport Power BI"
        steps:
          - script: |
              curl -X POST "https://api.powerbi.com/v1.0/myorg/groups/$(FABRIC_WORKSPACE_ID)/imports?name=LMS_Report" \
                -H "Authorization: Bearer $(PBI_ACCESS_TOKEN)" \
                -F "file=@reports/LMS_import.pbix"
            displayName: "D√©ployer le rapport Power BI"

  - stage: RefreshData
    displayName: "4Ô∏è‚É£ Rafra√Æchissement des donn√©es Power BI"
    jobs:
      - job: RefreshDataset
        displayName: "Rafra√Æchir le dataset Power BI"
        steps:
          - script: |
              curl -X POST \
                -H "Authorization: Bearer $(PBI_ACCESS_TOKEN)" \
                "https://api.powerbi.com/v1.0/myorg/groups/$(FABRIC_WORKSPACE_ID)/datasets/$(PBI_DATASET_ID)/refreshes"
            displayName: "Lancer le rafra√Æchissement du dataset"
```

---

## **üìå Explication des √©tapes**
| √âtape | Description |
|-------|-------------|
| **1Ô∏è‚É£ Transformation des donn√©es** | Ex√©cution des notebooks Fabric (**Raw ‚Üí Landing ‚Üí Bronze ‚Üí Silver ‚Üí Gold**). |
| **2Ô∏è‚É£ Stockage dans Fabric Lakehouse** | Chargement des donn√©es dans **Microsoft Fabric OneLake**. |
| **3Ô∏è‚É£ D√©ploiement du rapport Power BI** | Publication automatique de `LMS_import.pbix` sur Power BI Service. |
| **4Ô∏è‚É£ Rafra√Æchissement des donn√©es** | Mise √† jour des donn√©es du dataset Power BI. |

---

## **üéØ Personnalisation**
- **Notebooks Fabric** ‚Üí Ajoute les IDs de tes notebooks dans `FABRIC_NOTEBOOK_IDS` (s√©par√©s par des `,`).
- **Lakehouse Fabric** ‚Üí Change l‚ÄôID `FABRIC_LAKEHOUSE_ID`.
- **Workspace Power BI** ‚Üí Mets l‚ÄôID de ton **workspace** dans `FABRIC_WORKSPACE_ID`.
- **Dataset Power BI** ‚Üí Mets l‚ÄôID du **dataset Power BI** √† rafra√Æchir.

---

## **üöÄ Avantages de cette pipeline**
‚úÖ **Ex√©cute toute la cha√Æne Fabric (Notebooks, Lakehouse, Power BI)**.  
‚úÖ **Automatise le rafra√Æchissement des donn√©es Power BI**.  
‚úÖ **S‚Äôint√®gre parfaitement dans un workflow CI/CD avec Azure DevOps**.  

Dis-moi si tu veux adapter certaines parties ! üòä