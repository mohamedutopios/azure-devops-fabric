### **D√©ploiement Complet de Microsoft Fabric jusqu‚Äô√† Power BI avec Azure DevOps (CI/CD)**
L‚Äôobjectif ici est de **d√©ployer un pipeline analytique complet**, en passant par **Microsoft Fabric et Power BI**, en utilisant **Azure DevOps** pour automatiser le processus de bout en bout.

---

## **üìå Objectif du Projet**
üí° **Cas d‚Äôusage** : Analyse des ventes d‚Äôune entreprise.

Nous allons :
1. **Ingestion des donn√©es brutes** depuis un stockage Azure (Azure Blob Storage).
2. **Stockage et transformation** dans **Microsoft Fabric (Lakehouse + Notebook + Pipeline)**.
3. **Pr√©paration des donn√©es** avec **Dataflows**.
4. **Cr√©ation et publication d‚Äôun dashboard Power BI** connect√© aux donn√©es de Fabric.
5. **Automatisation du d√©ploiement** avec **Azure DevOps (CI/CD)**.

---

## **üõ†Ô∏è 1. Architecture et artefacts**
| üìÇ Dossier | Contenu |
|------------|---------|
| **Lakehouses/** | Stockage structur√© des ventes et clients |
| **Notebooks/** | Nettoyage et transformation des donn√©es |
| **Pipelines/** | Chargement des donn√©es brutes |
| **Dataflows/** | Pr√©paration des donn√©es pour Power BI |
| **PowerBI/** | Mod√®les et rapports Power BI |
| **azure-pipelines.yml** | Pipeline Azure DevOps pour l‚Äôautomatisation |

---

## **üìÇ 2. Contenu des fichiers**
### **1Ô∏è‚É£ `Lakehouses/sales_lakehouse.json`**
üìå D√©finit un **Lakehouse** avec **deux tables** : `sales` et `customers`.
```json
{
  "name": "SalesLakehouse",
  "description": "Stockage des ventes et des clients",
  "tables": [
    {
      "name": "sales",
      "path": "/tables/sales",
      "schema": [
        {"name": "id", "type": "integer"},
        {"name": "date", "type": "timestamp"},
        {"name": "customer_id", "type": "integer"},
        {"name": "amount", "type": "double"}
      ]
    }
  ]
}
```

---

### **2Ô∏è‚É£ `Notebooks/sales_transformation.ipynb`**
üìå Nettoie et transforme les ventes avec **PySpark**.
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("SalesTransformation").getOrCreate()

# Lecture des donn√©es brutes depuis le Lakehouse
df = spark.read.format("delta").load("/tables/sales")

# Nettoyage des donn√©es (filtrer les ventes n√©gatives)
df_cleaned = df.filter(df["amount"] > 0)

# √âcriture des donn√©es nettoy√©es
df_cleaned.write.format("delta").mode("overwrite").save("/tables/cleaned_sales")
```

---

### **3Ô∏è‚É£ `Pipelines/ingestion_pipeline.json`**
üìå Pipeline qui charge les ventes **depuis Azure Blob Storage vers Fabric**.
```json
{
  "name": "IngestionPipeline",
  "activities": [
    {
      "name": "Load Sales Data",
      "type": "CopyActivity",
      "inputs": [
        {
          "source": {
            "type": "BlobSource",
            "container": "sales-data",
            "path": "2025/",
            "format": "parquet"
          }
        }
      ],
      "outputs": [
        {
          "sink": {
            "type": "LakehouseSink",
            "path": "/tables/sales"
          }
        }
      ]
    }
  ]
}
```

---

### **4Ô∏è‚É£ `PowerBI/sales_dashboard.pbix`**
üìå Rapport Power BI **connect√© aux donn√©es du Lakehouse**.  
- **Graphiques** : Chiffre d'affaires total, ventes par r√©gion.  
- **Table de donn√©es** : Liste des transactions avec filtres dynamiques.  

---

## **üìÇ 3. Automatisation avec Azure DevOps**
### **üîπ Cr√©ation d‚Äôun Pipeline CI/CD**
Nous allons automatiser :
‚úÖ Le **d√©ploiement du Lakehouse, Notebook, Pipeline et Dataflow**.  
‚úÖ La **mise √† jour automatique du rapport Power BI** apr√®s transformation des donn√©es.  

---

### **üìù `azure-pipelines.yml` (Pipeline DevOps)**
```yaml
trigger:
  branches:
    include:
      - main

pool:
  vmImage: 'ubuntu-latest'

variables:
  FABRIC_WORKSPACE: "SalesWorkspace"
  FABRIC_TENANT_ID: "xxxxx-xxxx-xxxx-xxxx"
  FABRIC_CLIENT_ID: "xxxxx-xxxx-xxxx-xxxx"
  FABRIC_CLIENT_SECRET: "$(FABRIC_CLIENT_SECRET)"

steps:

# ‚úÖ Connexion Azure CLI
- task: AzureCLI@2
  displayName: "Authentification Azure"
  inputs:
    azureSubscription: 'MyAzureServiceConnection'
    scriptType: 'bash'
    scriptLocation: 'inlineScript'
    inlineScript: |
      az login --service-principal -u $(FABRIC_CLIENT_ID) -p $(FABRIC_CLIENT_SECRET) --tenant $(FABRIC_TENANT_ID)

# ‚úÖ D√©ploiement du Lakehouse
- task: AzureCLI@2
  displayName: "D√©ploiement du Lakehouse"
  inputs:
    scriptType: 'bash'
    scriptLocation: 'inlineScript'
    inlineScript: |
      az fabric deploy --workspace $(FABRIC_WORKSPACE) --artifact "./Lakehouses/sales_lakehouse.json"

# ‚úÖ Ex√©cution du Notebook de transformation
- task: AzureCLI@2
  displayName: "Ex√©cution du Notebook"
  inputs:
    scriptType: 'bash'
    scriptLocation: 'inlineScript'
    inlineScript: |
      az fabric run-notebook --workspace $(FABRIC_WORKSPACE) --notebook "./Notebooks/sales_transformation.ipynb"

# ‚úÖ D√©ploiement du Pipeline
- task: AzureCLI@2
  displayName: "D√©ploiement du Pipeline"
  inputs:
    scriptType: 'bash'
    scriptLocation: 'inlineScript'
    inlineScript: |
      az fabric deploy --workspace $(FABRIC_WORKSPACE) --artifact "./Pipelines/ingestion_pipeline.json"

# ‚úÖ Rafra√Æchir le Dataset Power BI
- task: PowerShell@2
  displayName: "Rafra√Æchir le Dataset Power BI"
  inputs:
    targetType: 'inline'
    script: |
      $workspaceId = "xxxx-xxxx-xxxx"
      $datasetId = "yyyy-yyyy-yyyy"
      $accessToken = $(PowerBI_AccessToken)

      Invoke-RestMethod -Uri "https://api.powerbi.com/v1.0/myorg/groups/$workspaceId/datasets/$datasetId/refreshes" `
      -Headers @{ "Authorization" = "Bearer $accessToken" } `
      -Method Post
```

---

## **üìä 4. D√©roulement du Processus CI/CD**
1Ô∏è‚É£ **D√©veloppeur pousse du code** (`git push origin main`).  
2Ô∏è‚É£ **Le pipeline Azure DevOps s‚Äôex√©cute** :  
   - D√©ploie **le Lakehouse, le Notebook, le Pipeline**.  
   - Lance **le traitement des donn√©es**.  
3Ô∏è‚É£ **Les donn√©es nettoy√©es sont mises √† jour dans Fabric**.  
4Ô∏è‚É£ **Power BI rafra√Æchit automatiquement son dataset**.  
5Ô∏è‚É£ **Les dashboards Power BI affichent les nouvelles donn√©es**.  

---

## **üöÄ R√©sultats attendus**
‚úÖ **Une cha√Æne analytique compl√®te et automatis√©e**.  
‚úÖ **Des donn√©es toujours √† jour dans Power BI** sans action manuelle.  
‚úÖ **Un d√©ploiement s√©curis√© et versionn√©** via Azure DevOps.  
‚úÖ **Une r√©duction des erreurs humaines et une acc√©l√©ration du traitement des donn√©es**.  

---

## **üéØ Prochaine √âtape : D√©mo Vid√©o**
Je vais capturer une vid√©o montrant :
- **La cr√©ation du pipeline dans Azure DevOps**.
- **L‚Äôex√©cution et le suivi des logs**.
- **La mise √† jour en temps r√©el de Power BI**.

Tu veux que j‚Äôajoute d‚Äôautres fonctionnalit√©s (ex : monitoring, logs avanc√©s) ? üöÄüòä